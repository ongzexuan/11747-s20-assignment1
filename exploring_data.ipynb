{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: topicclass/topicclass_train.txt\n",
      "Validation: topicclass/topicclass_valid.txt\n",
      "Test: topicclass/topicclass_test.txt\n"
     ]
    }
   ],
   "source": [
    "DIR = \"topicclass\"\n",
    "PREFIX = \"topicclass\"\n",
    "TRAIN_FILE = \"{}/{}_train.txt\".format(DIR, PREFIX)\n",
    "VALID_FILE = \"{}/{}_valid.txt\".format(DIR, PREFIX)\n",
    "TEST_FILE = \"{}/{}_test.txt\".format(DIR, PREFIX)\n",
    "\n",
    "print(\"Train: {}\".format(TRAIN_FILE))\n",
    "print(\"Validation: {}\".format(VALID_FILE))\n",
    "print(\"Test: {}\".format(TEST_FILE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Function to convert file to text, and then to labels and texts**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text(filename):\n",
    "    with open(filename, \"r\") as f:\n",
    "        return f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_cols(lines):\n",
    "    labels = []\n",
    "    texts = []\n",
    "    for line in lines:\n",
    "        tokens = line.split(\"|||\")\n",
    "        labels.append(tokens[0].strip())\n",
    "        texts.append(tokens[1].strip())\n",
    "    return labels, texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Get labels and texts**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'charmap' codec can't decode byte 0x90 in position 6609: character maps to <undefined>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-1bb3fbd590be>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_texts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msplit_cols\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mget_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mTRAIN_FILE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mvalid_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalid_texts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msplit_cols\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mget_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mVALID_FILE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mtest_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_texts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msplit_cols\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mget_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mTEST_FILE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Training size: {}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_labels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-0c480d04bf55>\u001b[0m in \u001b[0;36mget_text\u001b[1;34m(filename)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mget_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"r\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\encodings\\cp1252.py\u001b[0m in \u001b[0;36mdecode\u001b[1;34m(self, input, final)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mIncrementalDecoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIncrementalDecoder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfinal\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcharmap_decode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdecoding_table\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mStreamWriter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mCodec\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mStreamWriter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'charmap' codec can't decode byte 0x90 in position 6609: character maps to <undefined>"
     ]
    }
   ],
   "source": [
    "train_labels, train_texts = split_cols(get_text(TRAIN_FILE))\n",
    "valid_labels, valid_texts = split_cols(get_text(VALID_FILE))\n",
    "test_labels, test_texts = split_cols(get_text(TEST_FILE))\n",
    "\n",
    "print(\"Training size: {}\".format(len(train_labels)))\n",
    "print(\"Validation size: {}\".format(len(valid_labels)))\n",
    "print(\"Test size: {}\".format(len(test_labels)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remove top-k words**\n",
    "\n",
    "Removes top k words from dataset if `TOP_K` is specified. Note top k is determined from `train_dataset` only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOP_K = 20\n",
    "most_common_k_words = []\n",
    "\n",
    "# Removes top-k words\n",
    "if TOP_K > 0:\n",
    "    \n",
    "    counter = Counter()\n",
    "    for line in train_texts:\n",
    "        for w in line.split():\n",
    "            counter[w] += 1\n",
    "    \n",
    "    most_common_k_words = counter.most_common(TOP_K)\n",
    "\n",
    "print(most_common_k_words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Vocabulary and Label Sizes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total label set size: 16\n"
     ]
    }
   ],
   "source": [
    "train_labels_set = set(train_labels)\n",
    "valid_labels_set = set(train_labels)\n",
    "train_labels_set = set(train_labels)\n",
    "labels_set = train_labels_set.union(valid_labels_set, train_labels_set)\n",
    "print(\"Total label set size: {}\".format(len(labels_set)))\n",
    "print(labels_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train vocab set size: 137959\n",
      "Valid vocab set size: 4446\n",
      "Test vocab set size: 4548\n",
      "Total vocab set size: 138378\n",
      "Unseen validation vocab: 184\n",
      "Unseen test vocab: 235\n",
      "{'Pasig', 'Roxxi', 'Orevada', 'Dohrn', 'Lieberstein', '၁၂၇၇', 'Eichmanns', 'Asuma', 'Tut', 'jackrabbits', 'Associação', 'Demelza', 'gradations', 'Markowitz', 'mince', 'Nosek', 'Klea', 'Benchmark', 'RCTA', 'betrayer', 'Leaning', 'gammarus', 'Gapel', 'Bolan', 'Zhengyan', 'Devoted', 'Tagaung', '胡正言', 'Ranh', 'inclinata', 'Oriented', 'Olufunmilayo', 'Brasileira', 'electrocyclic', 'Homarus', 'Torv', 'Darlings', 'Stuckle', 'Strela', 'HNTB', 'sympathizing', 'aflatoxins', 'cyclopentenone', 'Bosses', 'Estell', 'palmetto', 'protic', 'Vagaland', 'Sarutobi', 'underplay', 'Dunder', 'starboard', 'Peavey', 'Champa', 'Lligwy', 'တရုတ', 'Akimichi', 'Perséphone', 'cyclopentenones', 'Heatherwick', 'cyclization', 'Careful', 'promisingly', '奈良', 'blowholes', '154th', 'Pinkner', 'Gaceta', 'Fled', 'Rainn', 'Overtoom', 'Krasinski', 'ဂို', 'Refueling', 'Linford', 'Ceasefire', '1273', 'Hongorai', 'Nazarov', '4401', 'Sudeikis', 'divinyl', 'Zhengmian', 'Trickle', 'Schrute', 'Theessink', 'jasmone', 'Hakon', 'Headlam', 'Lahontan', 'Burrafirth', 'Rhos', 'Eyton', 'boatloads', 'chickadee', 'Halpert', 'McDermitt', 'Merriwether', 'Stroker', 'Thelnetham', 'dope', 'millstones', 'Krab', 'Karoi', 'Wiese', '4π', 'dumpster', '၁၂၈၇', 'prostaglandins', 'Éva', 'Zamboanga', 'Bagan', 'Taruk', 'ZANLA', 'mechanistic', 'Dehong', 'Smethurst', 'Choji', 'Tamahori', 'Ino', 'Biggings', 'Housa', 'စစ', 'シカマル', 'Calligraphy', 'Kyawswa', 'hairstyles', 'fantail', 'allyl', 'tallgrass', 'Capturing', '朝日', 'progeria', 'fourteener', 'Kyla', 'Shikamaru', 'မာ', 'Cavite', 'Bourgeois', 'Deangelo', 'tiˈkäl', 'winded', 'Conal', 'Southaven', 'Seaville', 'မြန', 'Milione', 'Narathihapate', 'Ibuprofen', 'Kugler', 'Crepps', 'Hongguang', 'MAEC', 'cationic', 'Akin', 'Crofting', 'AM8', 'escarpments', 'freewheeling', 'Voe', 'Scharf', 'lobsters', 'ABPD', 'embossing', 'Strangest', 'Hopelessly', 'ZIPRA', 'planktonic', 'Mince', 'Hanlin', 'Snolda', 'Zachry', 'Surmising', 'Hunyani', 'tipsy', 'Crabbaberry', 'Mivo', 'Produtores', 'arpeggio', 'မွန', 'doubtfully', 'easygoing', 'Pseudoephedrine', 'Humpy'}\n"
     ]
    }
   ],
   "source": [
    "def get_vocab_set(lines):\n",
    "    s = set()\n",
    "    for line in lines:\n",
    "        for w in line.split():\n",
    "            s.add(w)\n",
    "    return s\n",
    "\n",
    "train_vocab_set = get_vocab_set(train_texts)\n",
    "valid_vocab_set = get_vocab_set(valid_texts)\n",
    "test_vocab_set = get_vocab_set(test_texts)\n",
    "\n",
    "total_vocab_set = train_vocab_set.union(valid_vocab_set, test_vocab_set)\n",
    "\n",
    "print(\"Train vocab set size: {}\".format(len(train_vocab_set)))\n",
    "print(\"Valid vocab set size: {}\".format(len(valid_vocab_set)))\n",
    "print(\"Test vocab set size: {}\".format(len(test_vocab_set)))\n",
    "print(\"Total vocab set size: {}\".format(len(total_vocab_set)))\n",
    "\n",
    "# Count unseen words\n",
    "unseen_valid = set()\n",
    "unseen_test = set()\n",
    "for w in valid_vocab_set:\n",
    "    if w not in train_vocab_set:\n",
    "        unseen_valid.add(w)\n",
    "for w in test_vocab_set:\n",
    "    if w not in train_vocab_set:\n",
    "        unseen_test.add(w)\n",
    "\n",
    "print(\"Unseen validation vocab: {}\".format(len(unseen_valid)))\n",
    "print(\"Unseen test vocab: {}\".format(len(unseen_test)))\n",
    "\n",
    "print(unseen_valid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
