{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple CNN Model\n",
    "\n",
    "Roughly the same implementation as Yoon Kim (2015)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import sys\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from collections import defaultdict\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils import data\n",
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# Check CUDA\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Program Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "SOURCE_PREFIX = \"google_news_negative300\"\n",
    "#SOURCE_PREFIX = \"glove_42b\"\n",
    "#SOURCE_PREFIX = \"numberbatch_en1908\"\n",
    "#SOURCE_PREFIX = \"freebase_skipgram1000\"\n",
    "VECTOR_SOURCE = \"{}_vectors.npy\".format(SOURCE_PREFIX)\n",
    "VOCAB_SOURCE = \"{}_words.txt\".format(SOURCE_PREFIX)\n",
    "VECTOR_SIZE = 300\n",
    "\n",
    "RAW_SOURCE_FOLDER = \"topicclass\"\n",
    "\n",
    "TRAIN_FILE = \"topicclass_train.txt\"\n",
    "VALID_FILE = \"topicclass_valid.txt\"\n",
    "TEST_FILE = \"topicclass_test.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hard-coded Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of CLASSES = 16\n"
     ]
    }
   ],
   "source": [
    "# Hard code a list of labels so we can map each label to an integer index\n",
    "hardcoded_labels = [\"Agriculture, food and drink\",\n",
    "                    \"Art and architecture\",\n",
    "                    \"Engineering and technology\",\n",
    "                    \"Geography and places\",\n",
    "                    \"History\",\n",
    "                    \"Language and literature\",\n",
    "                    \"Mathematics\",\n",
    "                    \"Media and drama\",\n",
    "                    \"Miscellaneous\",\n",
    "                    \"Music\",\n",
    "                    \"Natural sciences\",\n",
    "                    \"Philosophy and religion\",\n",
    "                    \"Social sciences and society\",\n",
    "                    \"Sports and recreation\",\n",
    "                    \"Video games\",\n",
    "                    \"Warfare\"]\n",
    "hardcoded_labels_dict = {l:i for i, l in enumerate(hardcoded_labels)}\n",
    "CLASSES = len(hardcoded_labels_dict)\n",
    "hardcoded_labels_dict[\"UNK\"] = -1\n",
    "print(\"Number of CLASSES = {}\".format(CLASSES))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a Function to Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_dataset(filename):\n",
    "    \n",
    "    labels = []\n",
    "    sentences = []\n",
    "    \n",
    "    with open(filename, \"r\", encoding=\"utf8\") as f:\n",
    "        \n",
    "        for line in f:\n",
    "            label, words = line.strip().split(\" ||| \")\n",
    "            labels.append(label)\n",
    "            sentences.append(words)\n",
    "    \n",
    "    return sentences, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Pre-trained Vector Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded vectors of shape: (3000000, 300)\n",
      "Loaded vocab of length: 3000000\n"
     ]
    }
   ],
   "source": [
    "# We now load from Numpy arrays instead\n",
    "pretrained_model = np.load(VECTOR_SOURCE)\n",
    "vocab = []\n",
    "with open(VOCAB_SOURCE, \"r\") as f:\n",
    "    for line in f:\n",
    "        vocab.append(line.strip())\n",
    "print(\"Loaded vectors of shape: {}\".format(pretrained_model.shape))\n",
    "print(\"Loaded vocab of length: {}\".format(len(vocab)))\n",
    "vocab_dict = {w:i for i, w in enumerate(vocab)}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MyDataset stores the raw text and labels as integers\n",
    "# When __getitem__ is called, it converts the requested item to a Tensor on the fly\n",
    "\n",
    "class MyDataset(data.Dataset):\n",
    "    \n",
    "    def __init__(self, sentences, labels, vocab_to_int, labels_to_int):\n",
    "        \n",
    "        self.sentences = [list(map(lambda x: vocab_to_int[x], sent.split())) for sent in sentences]\n",
    "        self.labels = [labels_to_int[l] for l in labels]  # map strings to ints\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return torch.LongTensor(self.sentences[index]), torch.LongTensor([self.labels[index]])\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Dataset and DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create collate function for padding\n",
    "# Adapted from tutorial at: https://suzyahyah.github.io/pytorch/2019/07/01/DataLoader-Pad-Pack-Sequence.html\n",
    "def pad_collate_fn(batch):\n",
    "    (xx, yy) = zip(*batch)\n",
    "    x_lens = torch.LongTensor([len(x) for x in xx])\n",
    "    xx_pad = pad_sequence(xx, batch_first=True, padding_value=0)\n",
    "    yy = torch.cat(yy)\n",
    "    \n",
    "    return xx_pad, x_lens, yy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOADER_PARAMS = {'batch_size': 64,\n",
    "                 'shuffle': True,\n",
    "                 'num_workers': 4,\n",
    "                 'collate_fn': pad_collate_fn}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sent, train_labels = read_dataset(\"{}/{}\".format(RAW_SOURCE_FOLDER, TRAIN_FILE))\n",
    "valid_sent, valid_labels = read_dataset(\"{}/{}\".format(RAW_SOURCE_FOLDER, VALID_FILE))\n",
    "test_sent, test_labels = read_dataset(\"{}/{}\".format(RAW_SOURCE_FOLDER, TEST_FILE))\n",
    "\n",
    "# Generate vocab list\n",
    "data_vocab = set()\n",
    "for source in [train_sent, valid_sent, test_sent]:\n",
    "    for sent in source:\n",
    "        for token in sent.split():\n",
    "            data_vocab.add(token)\n",
    "data_vocab = list(data_vocab)\n",
    "vocab_to_index = {v:i for i, v in enumerate(data_vocab)}\n",
    "\n",
    "# Define data loaders\n",
    "train_loader = data.DataLoader(MyDataset(train_sent, train_labels, vocab_to_index, hardcoded_labels_dict), **LOADER_PARAMS)\n",
    "valid_loader = data.DataLoader(MyDataset(valid_sent, valid_labels, vocab_to_index, hardcoded_labels_dict), **LOADER_PARAMS)\n",
    "test_loader = data.DataLoader(MyDataset(test_sent, test_labels, vocab_to_index, hardcoded_labels_dict), **LOADER_PARAMS)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct Embedding Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(138380, 300)\n"
     ]
    }
   ],
   "source": [
    "# Create the weight matrix for the embedding layer in the network,\n",
    "# taking into account unseen words\n",
    "\n",
    "embedding_matrix = []\n",
    "for i, word in enumerate(data_vocab):\n",
    "    if word in vocab_dict.keys():\n",
    "        embedding_matrix.append(pretrained_model[vocab_dict[word]])\n",
    "    else:\n",
    "        embedding_matrix.append(np.random.uniform(-0.1, 0.1, VECTOR_SIZE).astype(\"float32\"))\n",
    "        \n",
    "embedding_matrix.append(np.random.uniform(-0.1, 0.1, VECTOR_SIZE).astype(\"float32\")) # UNK\n",
    "embedding_matrix.append(np.zeros(VECTOR_SIZE).astype(\"float32\")) # Padding\n",
    "embedding_matrix = np.array(embedding_matrix)\n",
    "print(embedding_matrix.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyCNN(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, **kwargs):\n",
    "        super(MyCNN, self).__init__()\n",
    "        \n",
    "        self.MODEL = kwargs[\"MODEL\"]\n",
    "        self.VOCAB_SIZE = kwargs[\"VOCAB_SIZE\"]\n",
    "        self.CLASSES = kwargs[\"CLASSES\"]\n",
    "        self.WEIGHT_MATRIX = kwargs[\"WEIGHT_MATRIX\"]\n",
    "        self.DROPOUT = kwargs[\"DROPOUT\"]\n",
    "        \n",
    "        # Create embedding layer, copy weights from pretrained model if any\n",
    "        self.embedding = nn.Embedding(self.VOCAB_SIZE + 2, VECTOR_SIZE, padding_idx=self.VOCAB_SIZE + 1)\n",
    "        self.embedding.weight.requires_grad=False # do not train\n",
    "        self.embedding.weight.data.copy_(torch.from_numpy(self.WEIGHT_MATRIX)) # copy pre-trained weights\n",
    "        \n",
    "        # Create conv layers\n",
    "        self.filter_sizes = [3, 4, 5]\n",
    "        self.filter_numbers = [150, 150, 150]\n",
    "        self.reductions = [i - 1 for i in self.filter_sizes]\n",
    "        self.convs = nn.ModuleList([nn.Conv2d(1, n, (s, kwargs[\"DIMENSIONS\"]), stride=1) for s, n in zip(self.filter_sizes, self.filter_numbers)])\n",
    "        \n",
    "        # Create final classfier\n",
    "        self.fc = nn.Linear(sum(self.filter_numbers), self.CLASSES)\n",
    "        \n",
    "    def forward(self, input, input_lens, device, debug):\n",
    "        \n",
    "        b, max_len = input.shape\n",
    "        input = input.long()\n",
    "        \n",
    "        # Embed\n",
    "        x = self.embedding(input).unsqueeze(1) # add one channel\n",
    "        \n",
    "        # Create mask\n",
    "        mask = torch.arange(max_len).expand(len(input_lens), max_len).to(device) < input_lens.unsqueeze(1)\n",
    "\n",
    "        # DEBUG: MASK CHECK\n",
    "#         for i in range(len(mask)):\n",
    "#             nonzeros_mask = (mask[i] == 1).sum()\n",
    "#             nonzeros_len = input_lens[i]\n",
    "#             print(\"input_lens[{}]: {}, mask[{}]: {}\".format(i, nonzeros_len, i, nonzeros_mask))\n",
    "                    \n",
    "        # Apply conv layers\n",
    "        conved = [self.convs[i](x).squeeze().permute(1, 0, 2) for i in range(len(self.convs))]\n",
    "            \n",
    "        # Zero-out masked elements\n",
    "        masked_conved = [(conved[i] * mask[:,:-self.reductions[i]]).permute(1, 0, 2) for i in range(len(conved))]\n",
    "\n",
    "        # Max pool\n",
    "        pooled = [F.max_pool1d(F.relu(conv), kernel_size=conv.shape[-1]).squeeze() for conv in masked_conved]\n",
    "            \n",
    "        # Cat\n",
    "        cat = torch.cat(pooled, 1)\n",
    "        cat = F.dropout(cat, p=self.DROPOUT, training=self.training)\n",
    "        \n",
    "        # Fully connected\n",
    "        out = self.fc(cat)\n",
    "        \n",
    "        return out\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Train and Test Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(network, optimizer, criterion, network_params, train_params):\n",
    "    \n",
    "    # Set mode\n",
    "    network.train()\n",
    "    \n",
    "    # Load model if necessary\n",
    "    if train_params[\"LOAD_MODEL\"]:\n",
    "        pass  # Implement\n",
    "    \n",
    "    for epoch in range(train_params[\"START_EPOCH\"], train_params[\"END_EPOCH\"]):\n",
    "        \n",
    "        avg_loss = 0.0\n",
    "        for batch_num, (data, data_lens, labels) in enumerate(train_params['TRAIN_DATALOADER']):\n",
    "            \n",
    "            # Push data and labels to the GPU if available\n",
    "            data = data.to(train_params[\"DEVICE\"])\n",
    "            data_lens = data_lens.to(train_params[\"DEVICE\"])\n",
    "            labels = labels.to(train_params[\"DEVICE\"])\n",
    "            \n",
    "            # Zero the optimizer\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Generate the output\n",
    "            output = network(data, data_lens, train_params[\"DEVICE\"], train_params[\"DEBUG_PRINT\"])\n",
    "            \n",
    "            # Compute the loss and propagate\n",
    "            loss = criterion(output, labels.long())            \n",
    "            loss.backward()\n",
    "            avg_loss += loss.item() # Only for logging purposes\n",
    "            \n",
    "            # Clip gradients (apparently it should be here)\n",
    "            nn.utils.clip_grad_norm_(network.parameters(), train_params[\"GRADIENT_CLIP\"])\n",
    "            \n",
    "            # Step the optimizer\n",
    "            optimizer.step()             \n",
    "            \n",
    "            # Print every modulo PRINT_FREQ\n",
    "            if batch_num % train_params[\"PRINT_FREQ\"] == train_params[\"PRINT_FREQ\"] - 1:\n",
    "                print('Epoch: {}\\tBatch: {}\\tAvg-Loss: {:.6f}'.format(epoch + 1, batch_num + 1, avg_loss / train_params[\"PRINT_FREQ\"]))\n",
    "                avg_loss = 0.0                \n",
    "                \n",
    "            # Save memory\n",
    "            torch.cuda.empty_cache()\n",
    "            del data\n",
    "            del labels\n",
    "            del loss\n",
    "        \n",
    "        # Save model if specified\n",
    "        if train_params[\"SAVE_MODEL\"] and train_params['SAVE_MODEL_PREFIX'] and epoch == train_params['END_EPOCH'] - 1:\n",
    "            print(\"Saving model at epoch {}...\".format(epoch + 1))\n",
    "            torch.save(network.state_dict(), \"{}-model-{}.pt\".format(train_params['SAVE_MODEL_PREFIX'], epoch + 1))\n",
    "            torch.save(optimizer.state_dict(), \"{}-optim-{}.pt\".format(train_params['SAVE_MODEL_PREFIX'], epoch + 1))\n",
    "        \n",
    "        # Validate data at the end of one epoch\n",
    "        val_loss, val_acc = evaluate(network, train_params, mode=\"valid\")\n",
    "        train_loss, train_acc = evaluate(network, train_params, mode=\"train\")\n",
    "        print('Train Loss: {:.6f}\\tTrain Accuracy: {:.6f}\\tVal Loss: {:.6f}\\tVal Accuracy: {:.6f}'.\n",
    "              format(train_loss, train_acc, val_loss, val_acc))\n",
    "        print(\"\\n\")\n",
    "    \n",
    "def evaluate(network, train_params, mode=\"test\"):\n",
    "    \n",
    "    # Set mode\n",
    "    network.eval()\n",
    "    \n",
    "    # Stats\n",
    "    test_loss = []\n",
    "    accuracy = 0\n",
    "    total = 0\n",
    "    \n",
    "    # Switch dataset depending on mode\n",
    "    loader = train_params[\"TRAIN_DATALOADER\"]\n",
    "    if mode == \"test\":\n",
    "        loader = train_params[\"TEST_DATALOADER\"]\n",
    "    elif mode == \"valid\":\n",
    "        loader = train_params[\"VALID_DATALOADER\"]\n",
    "    \n",
    "    for batch_num, (data, data_lens, labels) in enumerate(loader):\n",
    "    \n",
    "        # Push data and labels to the GPU if available\n",
    "        data = data.to(train_params[\"DEVICE\"])\n",
    "        data_lens = data_lens.to(train_params[\"DEVICE\"])\n",
    "        labels = labels.to(train_params[\"DEVICE\"])\n",
    "        output = network(data, data_lens, train_params[\"DEVICE\"], train_params[\"DEBUG_PRINT\"])\n",
    "        \n",
    "        # Convert output to labels\n",
    "        _, pred_labels = torch.max(F.softmax(output, dim=1), 1)\n",
    "        pred_labels = pred_labels.view(-1)\n",
    "        \n",
    "        # Compute loss for logging\n",
    "        loss = criterion(output, labels.long())\n",
    "        \n",
    "        accuracy += torch.sum(torch.eq(pred_labels, labels)).item()\n",
    "        total += len(labels)\n",
    "        test_loss.extend([loss.item()] * data.size()[0])\n",
    "        \n",
    "        # Save memory\n",
    "        del data\n",
    "        del labels\n",
    "        \n",
    "    return np.mean(test_loss), accuracy / total\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weight Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(m):\n",
    "    if type(m) == nn.Conv1d or type(m) == nn.Linear:\n",
    "        torch.nn.init.xavier_normal_(m.weight.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Network and Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "NETWORK_PARAMS = {}\n",
    "\n",
    "NETWORK_PARAMS['MODEL'] = pretrained_model\n",
    "NETWORK_PARAMS['DIMENSIONS'] = 300\n",
    "NETWORK_PARAMS['VOCAB_SIZE'] = len(data_vocab)\n",
    "NETWORK_PARAMS['CLASSES'] = CLASSES\n",
    "NETWORK_PARAMS['DROPOUT'] = 0.5\n",
    "NETWORK_PARAMS['WEIGHT_MATRIX'] = embedding_matrix\n",
    "\n",
    "TRAIN_PARAMS = {}\n",
    "\n",
    "TRAIN_PARAMS['TRAIN_DATALOADER'] = train_loader\n",
    "TRAIN_PARAMS['VALID_DATALOADER'] = valid_loader\n",
    "TRAIN_PARAMS['TEST_DATALOADER'] = test_loader\n",
    "TRAIN_PARAMS['DEVICE'] = device\n",
    "TRAIN_PARAMS['START_EPOCH'] = 0\n",
    "TRAIN_PARAMS['END_EPOCH'] = 5\n",
    "TRAIN_PARAMS[\"PRINT_FREQ\"] = 1000\n",
    "TRAIN_PARAMS['LOAD_MODEL'] = False\n",
    "TRAIN_PARAMS['LOAD_MODEL_SOURCE'] = \"\"\n",
    "TRAIN_PARAMS[\"ADAMW_LEARNING_RATE\"] = 2e-3\n",
    "TRAIN_PARAMS[\"ADAMW_BETAS\"] = (0.9, 0.999)\n",
    "TRAIN_PARAMS[\"ADAMW_EPS\"] = 1e-08\n",
    "TRAIN_PARAMS[\"ADAMW_WEIGHT_DECAY\"] = 0\n",
    "TRAIN_PARAMS[\"GRADIENT_CLIP\"] = 3\n",
    "TRAIN_PARAMS['VECTOR_MODEL'] = pretrained_model\n",
    "TRAIN_PARAMS['SAVE_MODEL'] = True\n",
    "TRAIN_PARAMS['SAVE_MODEL_PREFIX'] = \"word2vec_150_static\"\n",
    "TRAIN_PARAMS['DEBUG_PRINT'] = True\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MyCNN(\n",
       "  (embedding): Embedding(138380, 300, padding_idx=138379)\n",
       "  (convs): ModuleList(\n",
       "    (0): Conv2d(1, 150, kernel_size=(3, 300), stride=(1, 1))\n",
       "    (1): Conv2d(1, 150, kernel_size=(4, 300), stride=(1, 1))\n",
       "    (2): Conv2d(1, 150, kernel_size=(5, 300), stride=(1, 1))\n",
       "  )\n",
       "  (fc): Linear(in_features=450, out_features=16, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize network and weights\n",
    "network = MyCNN(**NETWORK_PARAMS)\n",
    "network.apply(init_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize optimizer and criterion\n",
    "optimizer = torch.optim.AdamW(network.parameters(),\n",
    "                              lr=TRAIN_PARAMS[\"ADAMW_LEARNING_RATE\"],\n",
    "                              betas=TRAIN_PARAMS[\"ADAMW_BETAS\"],\n",
    "                              eps=TRAIN_PARAMS[\"ADAMW_EPS\"],\n",
    "                              weight_decay=TRAIN_PARAMS[\"ADAMW_WEIGHT_DECAY\"])\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actually Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\tBatch: 1000\tAvg-Loss: 0.996696\n",
      "Epoch: 1\tBatch: 2000\tAvg-Loss: 0.885159\n",
      "Epoch: 1\tBatch: 3000\tAvg-Loss: 0.860190\n",
      "Train Loss: 0.686035\tTrain Accuracy: 0.785336\tVal Loss: 0.635776\tVal Accuracy: 0.808709\n",
      "\n",
      "\n",
      "Epoch: 2\tBatch: 1000\tAvg-Loss: 0.653246\n",
      "Epoch: 2\tBatch: 2000\tAvg-Loss: 0.667188\n",
      "Epoch: 2\tBatch: 3000\tAvg-Loss: 0.676326\n",
      "Train Loss: 0.498449\tTrain Accuracy: 0.844362\tVal Loss: 0.569024\tVal Accuracy: 0.813375\n",
      "\n",
      "\n",
      "Epoch: 3\tBatch: 1000\tAvg-Loss: 0.499115\n",
      "Epoch: 3\tBatch: 2000\tAvg-Loss: 0.535241\n",
      "Epoch: 3\tBatch: 3000\tAvg-Loss: 0.547110\n",
      "Train Loss: 0.366544\tTrain Accuracy: 0.883253\tVal Loss: 0.664535\tVal Accuracy: 0.804044\n",
      "\n",
      "\n",
      "Epoch: 4\tBatch: 1000\tAvg-Loss: 0.348082\n",
      "Epoch: 4\tBatch: 2000\tAvg-Loss: 0.393928\n",
      "Epoch: 4\tBatch: 3000\tAvg-Loss: 0.431382\n",
      "Train Loss: 0.250029\tTrain Accuracy: 0.923914\tVal Loss: 0.766785\tVal Accuracy: 0.794712\n",
      "\n",
      "\n",
      "Epoch: 5\tBatch: 1000\tAvg-Loss: 0.245151\n",
      "Epoch: 5\tBatch: 2000\tAvg-Loss: 0.292813\n",
      "Epoch: 5\tBatch: 3000\tAvg-Loss: 0.324987\n",
      "Saving model at epoch 5...\n",
      "Train Loss: 0.195402\tTrain Accuracy: 0.936525\tVal Loss: 0.885378\tVal Accuracy: 0.776050\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "network.to(TRAIN_PARAMS['DEVICE'])\n",
    "train(network, optimizer, criterion, NETWORK_PARAMS, TRAIN_PARAMS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SOURCE_MODEL = \"word2vec_baseline_static-model-1.pt\"\n",
    "\n",
    "network = MyCNN(**NETWORK_PARAMS)\n",
    "network.load_state_dict(torch.load(SOURCE_MODEL))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "LABEL_LOADER_PARAMS = {'batch_size': 64,\n",
    "                       'shuffle': False,\n",
    "                       'num_workers': 4,\n",
    "                       'collate_fn': pad_collate_fn}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_loader = data.DataLoader(MyDataset(valid_sent, valid_labels, vocab_to_index, hardcoded_labels_dict), **LABEL_LOADER_PARAMS)\n",
    "test_loader = data.DataLoader(MyDataset(test_sent, test_labels, vocab_to_index, hardcoded_labels_dict), **LABEL_LOADER_PARAMS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label(network, train_params, loader):\n",
    "    \n",
    "    # Set mode\n",
    "    network.eval()\n",
    "    \n",
    "    # Stats\n",
    "    test_loss = []\n",
    "    accuracy = 0\n",
    "    total = 0\n",
    "    labels_output = []\n",
    "\n",
    "    for batch_num, (data, data_lens, labels) in enumerate(loader):\n",
    "    \n",
    "        # Push data and labels to the GPU if available\n",
    "        data = data.to(train_params[\"DEVICE\"])\n",
    "        data_lens = data_lens.to(train_params[\"DEVICE\"])\n",
    "        labels = labels.to(train_params[\"DEVICE\"])\n",
    "        output = network(data, data_lens, train_params[\"DEVICE\"], train_params[\"DEBUG_PRINT\"])\n",
    "        \n",
    "        # Convert output to labels\n",
    "        _, pred_labels = torch.max(F.softmax(output, dim=1), 1)\n",
    "        pred_labels = pred_labels.view(-1)\n",
    "        \n",
    "        #print(pred_labels)\n",
    "        for i in range(len(pred_labels)):\n",
    "            labels_output.append(hardcoded_labels[pred_labels[i]])\n",
    "        \n",
    "        # Save memory\n",
    "        del data\n",
    "        del labels\n",
    "        \n",
    "    return labels_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labelled 697 lines\n"
     ]
    }
   ],
   "source": [
    "network.to(TRAIN_PARAMS['DEVICE'])\n",
    "output_labels = label(network, TRAIN_PARAMS, test_loader)\n",
    "print(\"Labelled {} lines\".format(len(output_labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "LABELS_FILE = \"my_dev_labels_errors.txt\"\n",
    "with open(LABELS_FILE, \"w\") as l:\n",
    "    for word in output_labels:\n",
    "        l.write(word)\n",
    "        l.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
